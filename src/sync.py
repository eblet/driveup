# -*- coding: utf-8 -*-

import logging
import time
from pathlib import Path
from typing import Dict, Optional, Any, Set, Tuple
import random

from googleapiclient.discovery import Resource
from googleapiclient.errors import HttpError
import gspread

from . import config
from . import utils
from . import state_manager
from . import file_processor

log = logging.getLogger(__name__)

def process_drive(
    drive_service: Resource,
    gspread_client: Optional[gspread.Client],
    drive_id: Optional[str],
    drive_name: str,
    drive_backup_dir: Path,
    drive_state_dir: Path,
    processed_shared_drive_ids: Set[str],
    incremental_flag: bool,
    dry_run: bool
) -> Tuple[int, int, int, int, str]:
    """
    Process a single drive (My Drive or Shared Drive).
    Returns (processed_count, downloaded_count, deleted_count, failed_count, actual_mode).
    """
    log.info(f"Processing drive: {drive_name} (ID: {drive_id if drive_id else 'My Drive'})")
    
    # Initialize counters
    processed_count = 0
    downloaded_count = 0
    deleted_count = 0
    failed_count = 0
    
    # Setup state files
    state_file = drive_state_dir / (config.STATE_MAP_DRY_RUN_FILENAME if dry_run else config.STATE_MAP_FILENAME)
    token_file = drive_state_dir / config.START_TOKEN_FILENAME
    
    # Load state map. If incremental, also try to load token.
    state_map = state_manager.load_drive_state(state_file)
    start_token = None
    needs_full_sync = False
    
    if incremental_flag:
        start_token = state_manager.load_start_page_token(token_file)
        if not start_token:
            log.warning(f"No start token found for {drive_name}. Performing full sync.")
            needs_full_sync = True
            # Clear potentially stale state map if token is missing
            state_map = {}
            if state_file.exists():
                 log.warning("Deleting old state map %s before full sync (token missing).", state_file)
                 try: state_file.unlink()
                 except OSError as e: log.error("Failed to delete old state map: %s", e)
        else:
             log.info(f"Found start token for {drive_name}. Proceeding with incremental sync.")
    else:
        log.warning(f"Incremental flag not set for {drive_name}. Performing full sync.")
        needs_full_sync = True
        # Clear state and token files for a clean full sync
        state_map = {}
        if state_file.exists():
             log.warning("Deleting old state map %s before forced full sync.", state_file)
             try: state_file.unlink()
             except OSError as e: log.error("Failed to delete old state map: %s", e)
        if token_file.exists():
            log.warning("Deleting old start token %s before forced full sync.", token_file)
            try: token_file.unlink(missing_ok=True)
            except OSError as e: log.error("Failed to delete old start token file: %s", e)

    # --- Perform Sync ---    
    if needs_full_sync:
        # Full sync mode
        log.info(f"Starting full sync for {drive_name} using files.list")
        try:
            # Perform the full sync which populates the state_map
            processed, downloaded, deleted, failed = perform_full_sync(
                drive_service=drive_service,
                gspread_client=gspread_client,
                drive_id=drive_id,
                drive_name=drive_name,
                drive_backup_dir=drive_backup_dir,
                state_map=state_map, # Pass the map to be populated
                processed_shared_drive_ids=processed_shared_drive_ids,
                dry_run=dry_run
            )
            
            processed_count += processed
            downloaded_count += downloaded
            # deleted_count is 0 from perform_full_sync
            failed_count += failed
            
            # After successful full sync, get the initial start token for the *next* run
            if failed_count == 0: # Only get token if sync seemed okay
                new_start_token = state_manager.get_initial_start_page_token(drive_service, drive_id)
                if new_start_token:
                    if not dry_run:
                        # Save the populated state map and the new start token
                        state_manager.save_drive_state(state_map, state_file) 
                        state_manager.save_start_page_token(new_start_token, token_file)
                        log.info(f"Full sync for {drive_name} completed. State and new token saved.")
                    else:
                        # Save state map (potentially with size info if perform_full_sync added it), but not token
                        # Note: Current perform_full_sync doesn't calculate size, but save_drive_state handles it if passed.
                        state_manager.save_drive_state(state_map, state_file, total_size_bytes=None)
                        log.info(f"Full sync (DRY RUN) for {drive_name} completed. State map saved, token not saved.")
                else:
                     log.error(f"Failed to get initial start token after full sync for {drive_name}. Next run will require another full sync.")
                     # Still save the state we got, even if the token failed
                     state_manager.save_drive_state(state_map, state_file)
                     failed_count += 1 # Add a failure for the token fetch
            else:
                log.error(f"Full sync for {drive_name} encountered errors ({failed_count}). State may be incomplete. Token not fetched/saved.")
                # Save potentially incomplete state anyway for debugging?
                state_manager.save_drive_state(state_map, state_file)
                
        except HttpError as e:
            # Handle errors during the full sync process itself (e.g., auth errors)
            if e.resp.status == 401:
                log.error(f"Authorization error during full sync for {drive_name}. Please re-authenticate.")
                raise # Re-raise to stop the main loop
            else:
                log.error(f"API error during full sync for {drive_name}: {e}")
                failed_count += 1
        except Exception as e:
            log.error(f"Error during full sync for {drive_name}: {e}", exc_info=True)
            failed_count += 1
            
    else: # Incremental sync mode (token was loaded successfully)
        log.info(f"Starting incremental sync for {drive_name} from token: {start_token[:10]}...")
        try:
            # Use process_changes for incremental sync
            processed, downloaded, deleted, failed = process_changes(
                drive_service=drive_service,
                gspread_client=gspread_client,
                drive_id=drive_id,
                drive_name=drive_name,
                drive_backup_dir=drive_backup_dir,
                state_map=state_map, # Pass the loaded state map
                start_token=start_token, # Pass the loaded token
                processed_shared_drive_ids=processed_shared_drive_ids,
                dry_run=dry_run
            )
            
            processed_count += processed
            downloaded_count += downloaded
            deleted_count += deleted
            failed_count += failed
            
            # process_changes should update the state_map directly
            # It also handles getting and saving the *new* start token internally if not dry_run
            # So, we just need to save the final state map after the loop finishes
            if not dry_run:
                state_manager.save_drive_state(state_map, state_file)
                log.info(f"Incremental sync for {drive_name} finished. Final state saved.")
                # Token saving is handled within process_changes loop
            else:
                 state_manager.save_drive_state(state_map, state_file)
                 log.info(f"Incremental sync (DRY RUN) for {drive_name} finished. State saved, token not saved.")

        except HttpError as e:
            if e.resp.status == 401:
                log.error(f"Authorization error during incremental sync for {drive_name}. Please re-authenticate.")
                raise # Re-raise to stop main loop
            elif e.resp.status == 404 and "pageToken not found" in str(e):
                log.error(f"Invalid page token during incremental sync for {drive_name}. Full sync needed on next run.")
                # State saving is skipped here, as the sync failed mid-way
                failed_count += 1
                # Don't re-raise here? Allow script to continue with other drives, but log the failure.
            else:
                log.error(f"API error during incremental sync for {drive_name}: {e}")
                failed_count += 1
                # Save state map as it is after the error?
                state_manager.save_drive_state(state_map, state_file)
        except Exception as e:
            log.error(f"Error during incremental sync for {drive_name}: {e}", exc_info=True)
            failed_count += 1
            # Save state map as it is after the error?
            state_manager.save_drive_state(state_map, state_file)
    
    actual_mode = "full" if needs_full_sync else "incremental"
    log.info(f"--- Finished processing for drive: {drive_name} --- Counts: Processed={processed_count}, Downloaded={downloaded_count}, Deleted={deleted_count}, Failed={failed_count}")
    return processed_count, downloaded_count, deleted_count, failed_count, actual_mode

def process_changes(
    drive_service: Resource,
    gspread_client: Optional[gspread.Client],
    drive_id: Optional[str],
    drive_name: str,
    drive_backup_dir: Path,
    state_map: Dict[str, Dict[str, Any]],
    start_token: str,
    processed_shared_drive_ids: Set[str],
    dry_run: bool
) -> Tuple[int, int, int, int]:
    """
    Process changes from the Drive API.
    Returns (processed_count, downloaded_count, deleted_count, failed_count).
    """
    processed_count = 0
    downloaded_count = 0
    deleted_count = 0
    failed_count = 0
    
    page_token = start_token
    while page_token:
        try:
            # Get changes
            changes_params = {
                "pageToken": page_token,
                "spaces": "drive",
                "fields": "nextPageToken, newStartPageToken, changes(fileId, time, file(id, name, mimeType, size, parents, trashed))",
                "supportsAllDrives": True,
                "includeItemsFromAllDrives": True
            }
            if drive_id:
                changes_params["driveId"] = drive_id
                
            changes_result = drive_service.changes().list(**changes_params).execute()
            changes = changes_result.get("changes", [])
            
            # Process each change
            for change in changes:
                processed_count += 1
                
                # Get file details
                file_details = change.get("file", {})
                if not file_details:
                    continue
                    
                file_id = file_details.get("id")
                if not file_id:
                    continue
                    
                # Skip if file is in a shared drive we've already processed
                if drive_id and drive_id in processed_shared_drive_ids:
                    continue
                    
                # --- Skip Shared Drive files when processing 'My Drive' incrementally ---
                is_my_drive_processing = drive_id is None
                shared_drive_id = file_details.get("driveId") if file_details else None # Get driveId from file_details if available

                # We only apply this logic if the change is NOT a deletion and we are in My Drive sync
                if is_my_drive_processing and shared_drive_id and not file_details.get("trashed", False):
                    if shared_drive_id in processed_shared_drive_ids:
                        log.debug(f"Skipping change for file '{file_details.get('name', file_id)}' during 'My Drive' sync because its Shared Drive {shared_drive_id} is processed separately.")
                        continue # Skip processing this change
                    else:
                        # Handle change for item belonging to a shared drive NOT processed separately
                        item_name = file_details.get("name", "_unnamed_")
                        log.warning(f"Change for item '{item_name}' ({file_id}) found during 'My Drive' sync belongs to Shared Drive {shared_drive_id} (NOT processed separately). Processing in '{config.SHARED_FILES_DIR_NAME}'.")
                        target_dir = config.SHARED_FILES_DIR / shared_drive_id
                        target_dir.mkdir(parents=True, exist_ok=True)
                        target_path_base = target_dir / utils.sanitize_filename(item_name)
                        mime_type = file_details.get("mimeType")

                        # processed_count was already incremented at the start of the loop
                        if mime_type == config.FOLDER_MIME_TYPE:
                            try:
                                target_path_base.mkdir(parents=True, exist_ok=True)
                                downloaded_count += 1
                            except OSError as e:
                                log.error(f"Failed to create folder in Shared With Me dir: {target_path_base} - {e}")
                                failed_count += 1
                        elif mime_type:
                            # Download/update without adding to state map or S3
                            success, _ = file_processor.download_file(
                                service=drive_service,
                                item=file_details, # Use file_details from the change
                                local_path_base=target_path_base,
                                gspread_client=gspread_client
                            )
                            if success:
                                downloaded_count += 1
                            else:
                                failed_count += 1
                        else:
                            log.warning(f"Item '{item_name}' in Shared Drive {shared_drive_id} has no mimeType. Skipping change.")
                            failed_count += 1
                        continue # Skip normal processing and state map update
                #elif is_my_drive_processing and is_removed and file_id in some_way_to_track_shared_files:
                    # Handle deletion of a shared file - currently not implemented easily
                    # log.info(f"Deletion detected for item {file_id} potentially in Shared With Me. Manual cleanup may be needed.")

                # Handle file changes
                try:
                    if file_details.get("trashed", False):
                        # File was deleted
                        if file_id in state_map:
                            deleted_count += 1
                            del state_map[file_id]
                            log.info(f"Deleted file: {file_details.get('name', file_id)}")
                    else:
                        # File was modified or created
                        file_name = file_details.get("name", "_unnamed_")
                        mime_type = file_details.get("mimeType", "")
                        
                        # Skip folders in dry run
                        if dry_run and mime_type == config.FOLDER_MIME_TYPE:
                            continue
                            
                        # Get or create local path
                        local_path = file_processor.reconstruct_and_create_path(
                            service=drive_service,
                            item_id=file_id,
                            item_name=file_name,
                            item_parents=file_details.get("parents", []),
                            drive_id=drive_id,
                            drive_backup_dir=drive_backup_dir
                        )
                        
                        if not local_path:
                            log.error(f"Failed to get local path for {file_name}")
                            failed_count += 1
                            continue
                            
                        # Download file
                        success, final_path = file_processor.download_file(
                            service=drive_service,
                            item=file_details,
                            local_path_base=local_path,
                            gspread_client=gspread_client
                        )
                        
                        if success:
                            downloaded_count += 1
                            # Update state map
                            state_map[file_id] = {
                                "path": str(final_path.relative_to(drive_backup_dir)),
                                "modifiedTime": change.get("time"),
                                "is_folder": mime_type == config.FOLDER_MIME_TYPE
                            }
                            log.info(f"Downloaded/updated: {file_name}")
                        else:
                            failed_count += 1
                            log.error(f"Failed to download/update: {file_name}")
                            
                except HttpError as e:
                    if e.resp.status == 404:
                        log.warning(f"File not found (404): {file_details.get('name', file_id)}")
                        if file_id in state_map:
                            deleted_count += 1
                            del state_map[file_id]
                    else:
                        log.error(f"API error processing file {file_details.get('name', file_id)}: {e}")
                        failed_count += 1
                except Exception as e:
                    log.error(f"Error processing file {file_details.get('name', file_id)}: {e}", exc_info=True)
                    failed_count += 1
                    
            # Get next page token
            page_token = changes_result.get("nextPageToken")
            
            # Update start token for next run
            if "newStartPageToken" in changes_result:
                start_token = changes_result["newStartPageToken"]
                
        except HttpError as e:
            if e.resp.status == 401:
                log.error(f"Authorization error. Please re-authenticate.")
                raise
            elif e.resp.status == 404 and "pageToken not found" in str(e):
                log.error(f"Invalid page token. Full sync needed.")
                raise
            else:
                log.error(f"API error: {e}")
                failed_count += 1
                break
        except Exception as e:
            log.error(f"Error processing changes: {e}", exc_info=True)
            failed_count += 1
            break
            
    return processed_count, downloaded_count, deleted_count, failed_count

# --- Full Sync Function ---
def perform_full_sync(
    drive_service: Resource,
    gspread_client: Optional[gspread.Client],
    drive_id: Optional[str],
    drive_name: str,
    drive_backup_dir: Path,
    state_map: Dict[str, Dict[str, Any]], # Pass the state map to populate it
    processed_shared_drive_ids: Set[str], # To skip shared drive items during My Drive sync
    dry_run: bool = False
) -> Tuple[int, int, int, int]: # Returns processed, downloaded, deleted, failed counts
    """
    Performs a full sync by listing all files using files.list.
    Populates the state_map.
    Returns counts: (processed, downloaded, deleted, failed).
    """
    log.info(f"Performing full sync for drive '{drive_name}' using files.list... {'(DRY RUN)' if dry_run else ''}")
    processed_count = 0
    downloaded_count = 0
    deleted_count = 0 # Not applicable in full sync from scratch
    failed_count = 0
    all_items_map = {} # Temporary map to hold all fetched items {id: item}

    # --- 1. Fetch all items using files.list ---
    try:
        page_token = None
        log.info(f"Fetching full list of objects for drive: '{drive_name}'")
        # Base query parameters
        list_params = {
            "pageSize": 1000,
            "q": "trashed = false",
            "fields": "nextPageToken, files(id, name, parents, mimeType, modifiedTime, size, driveId)", # Removed shortcutDetails for now
            "orderBy": "folder, name",
            "supportsAllDrives": True,
            "includeItemsFromAllDrives": True
        }
        if drive_id:
            list_params["driveId"] = drive_id
            list_params["corpora"] = "drive"
        else:
            list_params["corpora"] = "user" # For My Drive

        # Fetch loop
        while True:
            if page_token: list_params["pageToken"] = page_token
            else: list_params.pop("pageToken", None)

            results = drive_service.files().list(**list_params).execute()
            items = results.get("files", [])

            for item in items:
                # Skip shortcuts (though field wasn't requested, good practice)
                if item.get("mimeType") == "application/vnd.google-apps.shortcut":
                    continue

                # --- Filter out Shared Drive items when processing 'My Drive' ---
                is_my_drive_processing = drive_id is None
                item_belongs_to_shared_drive_id = item.get("driveId")
                if is_my_drive_processing and item_belongs_to_shared_drive_id:
                    if item_belongs_to_shared_drive_id in processed_shared_drive_ids:
                         log.debug(f"Skipping item '{item.get('name')}' ({item['id']}) during 'My Drive' full sync because its Shared Drive {item_belongs_to_shared_drive_id} is processed separately.")
                         continue # Skip this item, it belongs to a drive processed elsewhere
                    else:
                         # Handle item belonging to a shared drive NOT processed separately
                         item_name = item.get("name", "_unnamed_")
                         log.warning(f"Item '{item_name}' ({item['id']}) found during 'My Drive' sync belongs to Shared Drive {item_belongs_to_shared_drive_id} (NOT processed separately). Downloading to '{config.SHARED_FILES_DIR_NAME}'.")
                         target_dir = config.SHARED_FILES_DIR / item_belongs_to_shared_drive_id # Subfolder per drive ID
                         target_dir.mkdir(parents=True, exist_ok=True)
                         target_path_base = target_dir / utils.sanitize_filename(item_name)
                         mime_type = item.get("mimeType")

                         processed_count += 1 # Count as processed
                         if mime_type == config.FOLDER_MIME_TYPE:
                             try:
                                 target_path_base.mkdir(parents=True, exist_ok=True)
                                 downloaded_count += 1 # Count folder creation
                             except OSError as e:
                                 log.error(f"Failed to create folder in Shared With Me dir: {target_path_base} - {e}")
                                 failed_count += 1
                         elif mime_type:
                             # Download without adding to state map or uploading to S3
                             success, _ = file_processor.download_file(
                                 service=drive_service,
                                 item=item,
                                 local_path_base=target_path_base,
                                 gspread_client=gspread_client
                             )
                             if success:
                                 downloaded_count += 1
                             else:
                                 failed_count += 1
                         else:
                             log.warning(f"Item '{item_name}' in Shared Drive {item_belongs_to_shared_drive_id} has no mimeType. Skipping.")
                             failed_count += 1
                         continue # Skip normal processing and state map update

                # Store valid items for normal processing
                all_items_map[item["id"]] = item

            page_token = results.get("nextPageToken")
            if not page_token: break
        log.info(f"Found {len(all_items_map)} total objects for full sync on '{drive_name}'.")

    except HttpError as error:
        log.error(f"API error during full scan of '{drive_name}': {error}. Full sync aborted.", exc_info=True)
        return processed_count, downloaded_count, deleted_count, 1 # Return 1 failure
    except Exception as e:
        log.error(f"Unknown error during full scan of '{drive_name}': {e}. Full sync aborted.", exc_info=True)
        return processed_count, downloaded_count, deleted_count, 1 # Return 1 failure

    # --- 1.5 Item Sampling for Dry Run ---
    items_to_process_list = list(all_items_map.values())
    if dry_run:
        # Separate folders and files meeting size criteria
        folders = [item for item in items_to_process_list if item.get("mimeType") == config.FOLDER_MIME_TYPE]
        small_files = [
            item for item in items_to_process_list
            if item.get("mimeType") != config.FOLDER_MIME_TYPE and int(item.get("size", 0)) <= config.DRY_RUN_MAX_FILE_SIZE_BYTES
        ]
        # Sort small files by size (optional, but helps select smallest first)
        small_files.sort(key=lambda x: int(x.get("size", 0)))
        
        # Build the sampled list
        sampled_items = []
        # Sample some folders (e.g., up to half the sample size)
        sampled_items.extend(random.sample(folders, min(len(folders), config.DRY_RUN_SAMPLE_SIZE // 2)))
        # Fill remaining sample slots with the smallest files
        remaining_sample_size = config.DRY_RUN_SAMPLE_SIZE - len(sampled_items)
        if remaining_sample_size > 0:
            sampled_items.extend(small_files[:min(len(small_files), remaining_sample_size)])
        
        # Handle edge case: if no folders/small files, sample randomly from all items
        if not sampled_items and items_to_process_list:
            log.warning("[DRY RUN] No folders or small files found for sampling. Sampling randomly from all items.")
            sampled_items = random.sample(items_to_process_list, min(len(items_to_process_list), config.DRY_RUN_SAMPLE_SIZE))
            
        items_to_process_list = sampled_items # Replace the list with the sampled items
        log.info(f"[DRY RUN] Selected {len(items_to_process_list)} items for processing based on sampling rules.")

    # --- 2. Process all fetched (or sampled) items ---
    log.info(f"Processing {len(items_to_process_list)} items for full sync on '{drive_name}'...")

    for item in items_to_process_list:
        processed_count += 1
        item_id = item["id"]
        item_name = item.get("name", "_unnamed_")
        mime_type = item.get("mimeType")
        is_folder = mime_type == config.FOLDER_MIME_TYPE

        try:
            # Get local path using the reconstructor
            local_path_base = file_processor.reconstruct_and_create_path(
                service=drive_service,
                item_id=item_id,
                item_name=item_name,
                item_parents=item.get("parents"),
                drive_id=drive_id, # Pass the context drive_id
                drive_backup_dir=drive_backup_dir
            )

            if not local_path_base:
                log.error(f"Full Sync: Failed to get local path for {item_name} ({item_id}). Skipping.")
                failed_count += 1
                continue

            if is_folder:
                # Ensure the folder exists locally (reconstruct_and_create_path might create parents, but not the final one)
                if not local_path_base.exists():
                    local_path_base.mkdir(parents=True, exist_ok=True)
                elif not local_path_base.is_dir():
                     log.error(f"Full Sync: Path for folder {item_name} exists but is not a directory: {local_path_base}. Skipping.")
                     failed_count += 1
                     continue
                # Update state map for folder
                state_map[item_id] = {
                    "path": str(local_path_base.relative_to(drive_backup_dir)),
                    "modifiedTime": item.get("modifiedTime"),
                    "is_folder": True
                }
                downloaded_count += 1 # Count folder creation as "downloaded" activity
                # No S3 action for folders

            else: # It's a file
                # Download/Export the file (includes potential S3 upload)
                # download_file handles adding the extension
                success, final_local_path = file_processor.download_file(
                    service=drive_service,
                    item=item,
                    local_path_base=local_path_base, # Pass the path without extension initially
                    gspread_client=gspread_client
                )

                if success:
                    downloaded_count += 1
                    # Update state map for file using the final path
                    state_map[item_id] = {
                        "path": str(final_local_path.relative_to(drive_backup_dir)),
                        "modifiedTime": item.get("modifiedTime"),
                        "is_folder": False
                    }
                else:
                    failed_count += 1
                    log.error(f"Full Sync: Failed to download/export file {item_name} ({item_id})")

        except Exception as e:
            log.error(f"Full Sync: Error processing item {item_name} ({item_id}): {e}", exc_info=True)
            failed_count += 1

    log.info(f"Full sync processing for '{drive_name}' finished.")
    return processed_count, downloaded_count, deleted_count, failed_count
